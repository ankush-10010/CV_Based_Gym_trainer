# -*- coding: utf-8 -*-
"""Copy of Yet another copy of SAM.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1p3uHYTiTLxdyhD9djiJXu7h8nsxfQk-u
"""

# from huggingface_hub import login
# login(token="qwertjbv")

!pip install supervision
!pip install -q torch torchvision torchaudio
!pip install -q transformers timm einops
!pip install -q ultralytics  # Easiest way to run SAM2 standalone
!pip install -q controlnet_aux mediapipe # For DWPose
!pip install -q opencv-python pillow accelerator
!pip install -q controlnet_aux onnxruntime-gpu huggingface_hub
!pip install -q torch torchvision torchaudio transformers timm einops
!pip install -q opencv-python pillow
!pip install -q controlnet_aux

!pip uninstall -y transformers
!pip install transformers==4.46.3

!pip install huggingface_hub==0.23.5

import transformers
print(transformers.__version__)



import os

BASE_DIR = os.path.abspath("Models")
os.makedirs(BASE_DIR, exist_ok=True)

FLORENCE_DIR = os.path.join(BASE_DIR, "Florence-2-large")
print("‚¨áÔ∏è Downloading Florence-2 Model...")
!pip install -q huggingface_hub
from huggingface_hub import snapshot_download
snapshot_download(repo_id="microsoft/Florence-2-large", local_dir=FLORENCE_DIR)

DWPOSE_DIR = os.path.join(BASE_DIR, "DWPose")
os.makedirs(DWPOSE_DIR, exist_ok=True)

print("‚¨áÔ∏è Downloading DWPose Models...")
!wget -q -nc -P {DWPOSE_DIR} https://huggingface.co/yzd-v/DWPose/resolve/main/yolox_l.onnx
!wget -q -nc -P {DWPOSE_DIR} https://huggingface.co/yzd-v/DWPose/resolve/main/dw-ll_ucoco_384.onnx

print(f"‚úÖ All models downloaded to: {BASE_DIR}")

!pip install dwpose

using_colab = True
if using_colab:
    import torch
    import torchvision
    print("PyTorch version:", torch.__version__)
    print("Torchvision version:", torchvision.__version__)
    print("CUDA is available:", torch.cuda.is_available())
    import sys
    !{sys.executable} -m pip install opencv-python matplotlib
    !{sys.executable} -m pip install 'git+https://github.com/facebookresearch/sam2.git'

    !mkdir -p images
    !wget -P images https://raw.githubusercontent.com/facebookresearch/sam2/main/notebooks/images/truck.jpg
    !wget -P images https://raw.githubusercontent.com/facebookresearch/sam2/main/notebooks/images/groceries.jpg

    !mkdir -p ../checkpoints/
    !wget -P ../checkpoints/ https://dl.fbaipublicfiles.com/segment_anything_2/092824/sam2.1_hiera_large.pt


import os
os.environ["PYTORCH_ENABLE_MPS_FALLBACK"] = "1"
import numpy as np
import torch
import matplotlib.pyplot as plt
from PIL import Image

if torch.cuda.is_available():
    device = torch.device("cuda")
elif torch.backends.mps.is_available():
    device = torch.device("mps")
else:
    device = torch.device("cpu")
print(f"using device: {device}")

if device.type == "cuda":
    torch.autocast("cuda", dtype=torch.bfloat16).__enter__()
    if torch.cuda.get_device_properties(0).major >= 8:
        torch.backends.cuda.matmul.allow_tf32 = True
        torch.backends.cudnn.allow_tf32 = True
elif device.type == "mps":
    print(
        "\nSupport for MPS devices is preliminary. SAM 2 is trained with CUDA and might "
        "give numerically different outputs and sometimes degraded performance on MPS. "
        "See e.g. https://github.com/pytorch/pytorch/issues/84936 for a discussion."
    )

np.random.seed(3)

def show_mask(mask, ax, random_color=False, borders = True):
    if random_color:
        color = np.concatenate([np.random.random(3), np.array([0.6])], axis=0)
    else:
        color = np.array([30/255, 144/255, 255/255, 0.6])
    h, w = mask.shape[-2:]
    mask = mask.astype(np.uint8)
    mask_image =  mask.reshape(h, w, 1) * color.reshape(1, 1, -1)
    if borders:
        import cv2
        contours, _ = cv2.findContours(mask,cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_NONE)
        # Try to smooth contours
        contours = [cv2.approxPolyDP(contour, epsilon=0.01, closed=True) for contour in contours]
        mask_image = cv2.drawContours(mask_image, contours, -1, (1, 1, 1, 0.5), thickness=2)
    ax.imshow(mask_image)

def show_points(coords, labels, ax, marker_size=375):
    pos_points = coords[labels==1]
    neg_points = coords[labels==0]
    ax.scatter(pos_points[:, 0], pos_points[:, 1], color='green', marker='*', s=marker_size, edgecolor='white', linewidth=1.25)
    ax.scatter(neg_points[:, 0], neg_points[:, 1], color='red', marker='*', s=marker_size, edgecolor='white', linewidth=1.25)

def show_box(box, ax):
    x0, y0 = box[0], box[1]
    w, h = box[2] - box[0], box[3] - box[1]
    ax.add_patch(plt.Rectangle((x0, y0), w, h, edgecolor='green', facecolor=(0, 0, 0, 0), lw=2))

def show_masks(image, masks, scores, point_coords=None, box_coords=None, input_labels=None, borders=True):
    for i, (mask, score) in enumerate(zip(masks, scores)):
        plt.figure(figsize=(10, 10))
        plt.imshow(image)
        show_mask(mask, plt.gca(), borders=borders)
        if point_coords is not None:
            assert input_labels is not None
            show_points(point_coords, input_labels, plt.gca())
        if box_coords is not None:
            # boxes
            show_box(box_coords, plt.gca())
        if len(scores) > 1:
            plt.title(f"Mask {i+1}, Score: {score:.3f}", fontsize=18)
        plt.axis('off')
        plt.show()

!pip install "huggingface-hub>=0.23.2,<1.0"

import os
import sys
import torch
import cv2
import numpy as np
from PIL import Image, ImageDraw
from types import ModuleType
import huggingface_hub
import json
from dwpose import DwposeDetector


# PATHS
BASE_DIR = os.path.abspath("Models")
DWPOSE_YOLO_PATH = os.path.join(BASE_DIR, "DWPose", "yolox_l.onnx")
DWPOSE_MODEL_PATH = os.path.join(BASE_DIR, "DWPose", "dw-ll_ucoco_384.onnx")
FLORENCE_PATH = os.path.join(BASE_DIR, "Florence-2-large")

# SAM 2 PATHS (ADJUST THESE TO MATCH YOUR DIRECTORY STRUCTURE)
SAM2_CHECKPOINT = "../checkpoints/sam2.1_hiera_large.pt"
SAM2_CONFIG = "configs/sam2.1/sam2.1_hiera_l.yaml"

# PATCH HF DOWNLOAD
original_hf_download = huggingface_hub.hf_hub_download
def patched_hf_download(repo_id, filename, **kwargs):
    if "yolox_l" in filename:
        print(f"üõ°Ô∏è Patch: Loading Local YOLOX -> {DWPOSE_YOLO_PATH}")
        return DWPOSE_YOLO_PATH
    if "dw-ll_ucoco" in filename:
        print(f"üõ°Ô∏è Patch: Loading Local DWPose -> {DWPOSE_MODEL_PATH}")
        return DWPOSE_MODEL_PATH
    return original_hf_download(repo_id, filename, **kwargs)

huggingface_hub.hf_hub_download = patched_hf_download

# IMPORTS
from transformers import AutoProcessor, AutoModelForCausalLM
#from controlnet_aux import DWposeDetector

# SAM 2 IMPORTS
try:
    from sam2.build_sam import build_sam2
    from sam2.sam2_image_predictor import SAM2ImagePredictor
except ImportError:
    print("‚ùå Error: SAM 2 not installed. Please install facebook/sam2.")
    sys.exit(1)

# SETTINGS
IMAGE_PATH = "/content/good_bp_pose.jpeg"
TEXT_PROMPT = "rod"
DEVICE = "cuda" if torch.cuda.is_available() else "cpu"
OUTPUT_DIR = "output_results"
os.makedirs(OUTPUT_DIR, exist_ok=True)

print(f"üöÄ Loading models on {DEVICE}...")

# Load Florence
if os.path.exists(FLORENCE_PATH):
    florence_model = AutoModelForCausalLM.from_pretrained(FLORENCE_PATH, trust_remote_code=True, torch_dtype='auto').to(DEVICE).eval()
    florence_processor = AutoProcessor.from_pretrained(FLORENCE_PATH, trust_remote_code=True)
else:
    raise FileNotFoundError("Florence model not found.")

def step_1_florence(image, text_input):
    print(f"\n--- PART 1: FLORENCE DETECTION ('{text_input}') ---")

    task_prompt = '<CAPTION_TO_PHRASE_GROUNDING>'
    prompt = task_prompt + text_input

    # Inference
    inputs = florence_processor(text=prompt, images=image, return_tensors="pt").to(DEVICE, torch.float16)
    with torch.inference_mode():
        generated_ids = florence_model.generate(
            input_ids=inputs["input_ids"],
            pixel_values=inputs["pixel_values"],
            max_new_tokens=1024,
            do_sample=False,
            num_beams=3,
        )
    generated_text = florence_processor.batch_decode(generated_ids, skip_special_tokens=False)[0]
    parsed_answer = florence_processor.post_process_generation(
        generated_text, task=task_prompt, image_size=(image.width, image.height)
    )

    bboxes = parsed_answer.get(task_prompt, {}).get('bboxes', [])

    # Extract Numpy Coordinates for SAM
    box_prompt = None
    if bboxes:
        box_prompt = np.array(bboxes[0])
        print(f"   üéØ BOX COORDINATES FOR SAM: {box_prompt}")

    # VISUALIZATION
    debug_image = image.copy()
    draw = ImageDraw.Draw(debug_image)

    for box in bboxes:
        draw.rectangle(box, outline="red", width=4)

    save_path = os.path.join(OUTPUT_DIR, "step_1_florence_debug.png")
    debug_image.save(save_path)
    print(f"   ‚úÖ Florence detected {len(bboxes)} boxes.")

    return bboxes, box_prompt

def step_2_sam(original_image_obj, box_prompt):
    """
    Replaced function using SAM2ImagePredictor logic.
    Args:
        original_image_obj: The PIL image object passed from main.
        box_prompt: Numpy array [x1, y1, x2, y2] from Florence.
    """
    print(f"\n--- PART 2: SAM 2 SEGMENTATION (PREDICTOR) ---")

    if box_prompt is None:
        print("   ‚ùå No box prompt provided. Skipping SAM.")
        return None, None

    image_np = np.array(original_image_obj.convert("RGB"))

    print("   ‚öôÔ∏è Initializing SAM 2 Predictor...")
    try:
        sam2_model = build_sam2(SAM2_CONFIG, SAM2_CHECKPOINT, device=DEVICE)
        predictor = SAM2ImagePredictor(sam2_model)
    except Exception as e:
        print(f"   ‚ùå Failed to load SAM 2 model: {e}")
        return None, None

    # Set image
    predictor.set_image(image_np)
    print(f"   üéØ Using Box Prompt: {box_prompt}")

    masks, scores, _ = predictor.predict(
        point_coords=None,
        point_labels=None,
        box=box_prompt[None, :],
        multimask_output=False,
    )

    final_mask = masks[0]

    # Convert to binary image (0 = Black, 255 = White)
    binary_mask_image = (final_mask > 0).astype(np.uint8) * 255

    # SAVE: Binary Mask
    mask_save_path = os.path.join(OUTPUT_DIR, "step_2_binary_mask.png")
    Image.fromarray(binary_mask_image).save(mask_save_path)
    print(f"   üíæ Saved Binary Mask: {mask_save_path}")
-
    # Apply Canny Edge Detection
    edges = cv2.Canny(binary_mask_image, 100, 200)

    # SAVE: Raw Edges
    raw_edge_path = os.path.join(OUTPUT_DIR, "step_2_raw_edges.png")
    Image.fromarray(edges).save(raw_edge_path)
    print(f"   üíæ Saved Raw Edges: {raw_edge_path}")

    # Thicken Lines (Dilation)
    thickness_level = 2
    if thickness_level > 1:
        kernel_size = thickness_level
        kernel = np.ones((kernel_size, kernel_size), np.uint8)
        thick_edges = cv2.dilate(edges, kernel, iterations=1)
    else:
        thick_edges = edges

    # Convert to PIL for return/saving
    final_canny_pil = Image.fromarray(thick_edges, mode="L").convert("RGB")

    # SAVE: Thick Edges
    canny_save_path = os.path.join(OUTPUT_DIR, "step_2_canny_final.png")
    final_canny_pil.save(canny_save_path)
    print(f"   üíæ Saved Final Canny: {canny_save_path}")

    return binary_mask_image, final_canny_pil

def step_3_dwpose(image):
    # 1. Setup Output Path
    output_image_path = "step_3_skeleton.png"

    # 2. Initialize the Model
    print("Loading Model...")
    model = DwposeDetector.from_pretrained_default()

    # 3. Run Inference
    print("Extracting pose...")
    # 'image' should be a PIL Image object here
    imgOut, keypoints, source = model(
        image,
        include_hand=True,
        include_face=False,
        include_body=True,
        image_and_json=True,
        detect_resolution=512
    )

    # 4. Save the Visual Output (Optional but good for debugging)
    imgOut.save(output_image_path)
    print(f"Done! \nSkeleton image saved to: {output_image_path}")

    return imgOut

def step_4_overlay(canny_image, pose_image=None):
    print(f"\n--- PART 4: OVERLAYING IMAGES ---")

    if pose_image is not None:
        if isinstance(pose_image, str):
            if pose_image.startswith("/") or pose_image.startswith("."):
                pose_image = Image.open(pose_image)
            else:
                try:
                    if "," in pose_image:
                        pose_image = pose_image.split(",")[1]

                    # Decode string to bytes
                    image_bytes = base64.b64decode(pose_image)

                    # Convert bytes to PIL Image
                    pose_image = Image.open(io.BytesIO(image_bytes))
                except Exception as e:
                    print(f"Error decoding base64: {e}")
                    return None


        print(f"   ü¶¥ Using DWPose Skeleton as Base")
        target_background = pose_image.convert("RGB")

    if target_background.size != canny_image.size:
        target_background = target_background.resize(canny_image.size, Image.LANCZOS)

    base_np = np.array(target_background)

    # Convert Canny to Grayscale for masking (ensure single channel)
    canny_np = np.array(canny_image.convert("L"))

    # Create a mask where the edges are white (pixel > 127)
    edge_mask = canny_np > 100

    base_np[edge_mask] = [255, 255, 255]

    final_overlay = Image.fromarray(base_np)
    save_path = os.path.join(OUTPUT_DIR, "step_4_final_overlay.png")
    final_overlay.save(save_path)

    print(f"   ‚úÖ Overlay saved to: {save_path}")
    return final_overlay

if __name__ == "__main__":
    # Load Original Image
    if os.path.exists(IMAGE_PATH):
        original_image = Image.open(IMAGE_PATH).convert("RGB")
    else:
        print(f"‚ùå Error: Image not found at {IMAGE_PATH}")
        sys.exit(1)

    # --- STEP 1: Florence (Get Box) ---
    bboxes, box_prompt = step_1_florence(original_image, TEXT_PROMPT)

    # --- STEP 2: SAM 2 (Get Canny) ---
    binary_mask, canny_result = step_2_sam(original_image, box_prompt)

    if canny_result is not None:

        pose_result = step_3_dwpose(original_image)

        final_result = step_4_overlay(canny_result, pose_image=pose_result)

        final_result.show()
    else:
        print("‚ùå Pipeline stopped because Step 2 failed.")





